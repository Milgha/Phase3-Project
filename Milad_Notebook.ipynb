{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install uszipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --force-reinstall numpy==1.23.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy\n",
    "#numpy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.56.2-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in c:\\users\\milad\\documents\\flatiron\\anaconda\\envs\\learn-env\\lib\\site-packages (from numba) (5.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in c:\\users\\milad\\documents\\flatiron\\anaconda\\envs\\learn-env\\lib\\site-packages (from numba) (1.23.3)\n",
      "Requirement already satisfied: setuptools<60 in c:\\users\\milad\\documents\\flatiron\\anaconda\\envs\\learn-env\\lib\\site-packages (from numba) (57.4.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-win_amd64.whl (23.2 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\milad\\documents\\flatiron\\anaconda\\envs\\learn-env\\lib\\site-packages (from importlib-metadata; python_version < \"3.9\"->numba) (3.3.0)\n",
      "Installing collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.39.1 numba-0.56.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%timeit` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit, KFold, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "#import geopy\n",
    "#from geopy.geocoders import Nominatim\n",
    "#from arcgis.geocoding import reverse_geocode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crashes = pd.read_csv('data/Traffic_Crashes_-_Crashes.csv')\n",
    "df_crashes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crashes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes.TRAFFIC_CONTROL_DEVICE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping relevant Features\n",
    "df_crashes_drop = df_crashes [[\n",
    "    'CRASH_RECORD_ID',\n",
    "#    'RD_NO',\n",
    "    'CRASH_DATE',\n",
    "    'POSTED_SPEED_LIMIT',\n",
    "    'WEATHER_CONDITION',\n",
    "#    'LIGHTING_CONDITION',\n",
    "#    'FIRST_CRASH_TYPE',\n",
    "    'ROADWAY_SURFACE_COND',\n",
    "    'ROAD_DEFECT',\n",
    "#    'CRASH_TYPE',\n",
    "#    'DAMAGE',\n",
    "#    'PRIM_CONTRIBUTORY_CAUSE',\n",
    "#    'STREET_NAME',\n",
    "#    'NUM_UNITS',\n",
    "    'INJURIES_TOTAL',\n",
    "    'INJURIES_FATAL',\n",
    "    'CRASH_HOUR',\n",
    "    'CRASH_DAY_OF_WEEK',\n",
    "    'CRASH_MONTH',\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "#    'LOCATION',\n",
    "]]\n",
    "print(df_crashes_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crashes_drop.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Rows with Latitude & Longitude = 0\n",
    "df_crashes_drop = df_crashes_drop[df_crashes_drop['LATITUDE'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_drop['has_injuries'] = df_crashes_drop.INJURIES_TOTAL.apply(lambda x: int(x>0))\n",
    "df_crashes_drop['has_fatality'] = df_crashes_drop.INJURIES_FATAL.apply(lambda x: int(x>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of accidents by having injuries ( Alternatively we can get the same for fatality)\n",
    "crash_df_ = df_crashes_drop.groupby(by=['LONGITUDE','LATITUDE']).agg(crashes=('has_injuries', 'count'), has_injuries=('has_injuries', 'max')).reset_index()\n",
    "crash_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "fg = sns.scatterplot(data=crash_df_, x='LONGITUDE',y='LATITUDE',\n",
    "                hue='has_injuries', sizes={1:1,25:12.5,50:25,100:50,200:100}, size_norm=(1,100), size='crashes', alpha=.4)\n",
    "fg.set_axis_off()\n",
    "fg.set_title('Crashes in Chicago',fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crashes_drop.replace({'UNKNOWN':np.nan}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_ = df_crashes_drop.dropna(subset = 'LATITUDE', axis = 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles = pd.read_csv('data/Traffic_Crashes_-_Vehicles.csv')\n",
    "df_vehicles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping Relevant Features\n",
    "df_vehicles_drop = df_vehicles [[\n",
    "    'CRASH_RECORD_ID',\n",
    "#    'RD_NO',\n",
    "#    'CRASH_DATE',\n",
    "    'VEHICLE_ID',\n",
    "#    'MAKE',\n",
    "#    'MODEL',\n",
    "#    'LIC_PLATE_STATE',\n",
    "    'VEHICLE_YEAR',\n",
    "#    'VEHICLE_DEFECT',\n",
    "#    'VEHICLE_TYPE',\n",
    "#    'VEHICLE_USE',\n",
    "#    'TRAVEL_DIRECTION',\n",
    "    'OCCUPANT_CNT',\n",
    "#    'VEHICLE_CONFIG',\n",
    "]]\n",
    "print(df_vehicles_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles.VEHICLE_CONFIG.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_vehicles_drop.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dropping rows with vehicle year bigger than 2022 and below 1970\n",
    "df_vehicles_drop1 = df_vehicles_drop[(df_vehicles_drop['VEHICLE_YEAR'] >= 1970) & (df_vehicles_drop['VEHICLE_YEAR'] <= 2022) | (df_vehicles_drop['VEHICLE_YEAR'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "df_vehicles_drop1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "df_vehicles_drop1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Vehicles with 0 Occupant (Parked Cars)\n",
    "df_vehicles_w_occ = df_vehicles_drop1[df_vehicles_drop.OCCUPANT_CNT != 0.0]\n",
    "print(df_vehicles_w_occ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_vehicles_w_occ.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all UNKNOWN values to missing values\n",
    "df_vehicles_w_occ.replace({'UNKNOWN':np.nan}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people = pd.read_csv('data/Traffic_Crashes_-_PEOPLE.csv')\n",
    "df_people.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "df_people.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping Relevant Features\n",
    "df_people_drop = df_people [[\n",
    "    'CRASH_RECORD_ID',\n",
    "#    'RD_NO',\n",
    "#    'PERSON_ID',\n",
    "    'PERSON_TYPE',\n",
    "    'VEHICLE_ID',\n",
    "#    'CITY',\n",
    "#    'STATE',\n",
    "#    'ZIPCODE',\n",
    "    'SEX',\n",
    "    'AGE',\n",
    "#    'DRIVERS_LICENSE_STATE',\n",
    "#    'DRIVERS_LICENSE_CLASS',\n",
    "    'SAFETY_EQUIPMENT',\n",
    "    'AIRBAG_DEPLOYED',\n",
    "#    'EJECTION',\n",
    "#    'INJURY_CLASSIFICATION',\n",
    "#    'DRIVER_VISION',\n",
    "#    'DRIVER_ACTION',\n",
    "#    'PHYSICAL_CONDITION',\n",
    "#    'PEDPEDAL_ACTION',\n",
    "#    'PEDPEDAL_VISIBILITY',\n",
    "#    'PEDPEDAL_LOCATION',\n",
    "#    'BAC_RESULT',\n",
    "#    'BAC_RESULT VALUE',\n",
    "#    'CELL_PHONE_USE',\n",
    "]]\n",
    "print(df_people_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtering the data with Drivers only\n",
    "df_people_driver = df_people_drop[df_people_drop.PERSON_TYPE == 'DRIVER']\n",
    "print(df_people_driver.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_driver.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_people_driver.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with driver age bigger than 90 and below 18\n",
    "df_people_driver_age = df_people_driver[(df_people_driver['AGE'] >= 18) & (df_people_driver['AGE'] <= 90) | (df_people_driver['AGE'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all UNKNOWN values to missing values\n",
    "#Unknown = ['UNKNOWN','USAGE UNKNOWN', 'DEPLOYMENT UNKNOWN']\n",
    "df_vehicles_w_occ.replace({'Unknown' :np.nan , 'USAGE UNKNOWN' :np.nan , 'DEPLOYMENT UNKNOWN':np.nan }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df_crashes_drop.merge(df_vehicles_w_occ, on='CRASH_RECORD_ID')\n",
    "print(merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged.merge(df_people_driver_age, on=['VEHICLE_ID','CRASH_RECORD_ID'])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bins for times \n",
    "# 0-6 = Late Night/Early Morning \n",
    "# 6-12 = Morning \n",
    "# 12-18 = Afternoon/Rush Hour\n",
    "# 18-23 = Late Evening \n",
    "df['time_bins'] = pd.cut(x=df['CRASH_HOUR'], bins = [-1,6,12,18,24], \n",
    "                         labels = ['Late Night/Early Morning', \n",
    "                        'Morning', 'Afternoon/Rush Hour','Late Evening'])\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unnecassary Features\n",
    "df_relv = df.drop([\n",
    "    'CRASH_RECORD_ID',\n",
    "#    'RD_NO',\n",
    "#    'PERSON_ID',\n",
    "    'CRASH_DATE',\n",
    "    'VEHICLE_ID',\n",
    "#    'CITY',\n",
    "#    'STATE',\n",
    "#    'ZIPCODE',\n",
    "    'PERSON_TYPE',\n",
    "    'OCCUPANT_CNT',\n",
    "#    'has_injuries',\n",
    "#    'has_fatality',\n",
    "#    'LONGITUDE',\n",
    "#    'LATITUDE',\n",
    "    'ROAD_DEFECT',\n",
    "#    'LIC_PLATE_STATE',\n",
    "#    'TRAVEL_DIRECTION', \n",
    "#    'DRIVERS_LICENSE_STATE',\n",
    "#    'INJURY_CLASSIFICATION',\n",
    "#    'DRIVER_ACTION',\n",
    "#    'PHYSICAL_CONDITION'],\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_drop_missing = df_relv.dropna(subset = ['LATITUDE','LONGITUDE'], axis = 0).reset_index(drop=True)\n",
    "df_drop_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_drop_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_drop_missing[['LATITUDE',\"LONGITUDE\"]]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 50, init ='k-means++')\n",
    "kmeans.fit(df1[df1.columns]) # Compute k-means clustering.\n",
    "df1['cluster_label'] = kmeans.fit_predict(df1[df1.columns])\n",
    "#centers = kmeans.cluster_centers_ # Coordinates of cluster centers.\n",
    "#labels = kmeans.predict(df1[df1.columns]) # Labels of each point\n",
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sns.scatterplot(x=df1.LATITUDE, y = df1.LONGITUDE, hue=df1.cluster_label, palette = 'rocket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "fg = sns.scatterplot(data=df1, x='LONGITUDE',y='LATITUDE', palette = 'viridis',\n",
    "                hue='cluster_label', size_norm=(1,100), alpha=.4)\n",
    "fg.set_axis_off()\n",
    "fg.set_title('Crashes in Chicago',fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.plot.scatter(x = 'LATITUDE', y = 'LONGITUDE', c=df1.cluster_label, s=50, cmap='viridis')\n",
    "#plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_missing2 = df_drop_missing.dropna(subset= ['AGE','VEHICLE_YEAR','ROADWAY_SURFACE_COND','WEATHER_CONDITION'], axis = 0).reset_index(drop=True)\n",
    "print(df_drop_missing2.isna().sum())\n",
    "print(df_drop_missing2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting features into numeric and categorical\n",
    "numeric_columns = [\n",
    "    'POSTED_SPEED_LIMIT',\n",
    "#    'NUM_UNITS',\n",
    "    'VEHICLE_YEAR',\n",
    "#    'OCCUPANT_CNT',\n",
    "    'AGE',\n",
    "    \n",
    "]\n",
    "\n",
    "cat_columns = [\n",
    "    'WEATHER_CONDITION', \n",
    "    'ROADWAY_SURFACE_COND',  \n",
    "#    'ROAD_DEFECT',  \n",
    "#    'CRASH_HOUR',  \n",
    "    'CRASH_DAY_OF_WEEK',  \n",
    "    'CRASH_MONTH',   \n",
    "#    'ZIPCODE',  \n",
    "    'SEX',  \n",
    "    'SAFETY_EQUIPMENT', \n",
    "    'AIRBAG_DEPLOYED',  \n",
    "    'time_bins',\n",
    "]\n",
    "\n",
    "\n",
    "Target1 = df_drop_missing2[[\n",
    "    'has_injuries',           \n",
    "#    'has_fatality',    \n",
    "]]\n",
    "\n",
    "Target2 = df_drop_missing2[[\n",
    "#    'has_injuries',           \n",
    "    'has_fatality',    \n",
    "]]\n",
    "\n",
    "numeric_df = df_drop_missing2[numeric_columns]\n",
    "cat_df = df_drop_missing2[cat_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([numeric_df,cat_df] , axis = 1)\n",
    "y = Target1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 42)\n",
    "\n",
    "X_train_nums = X_train [[\n",
    "    'POSTED_SPEED_LIMIT',\n",
    "    'VEHICLE_YEAR',\n",
    "    'AGE',\n",
    "    \n",
    "]]\n",
    "\n",
    "X_train_cats = X_train [[  \n",
    "    'WEATHER_CONDITION', \n",
    "    'ROADWAY_SURFACE_COND',  \n",
    "#    'ROAD_DEFECT',  \n",
    "#    'CRASH_HOUR',  \n",
    "    'CRASH_DAY_OF_WEEK',  \n",
    "    'CRASH_MONTH',   \n",
    "#    'ZIPCODE',  \n",
    "    'SEX',  \n",
    "    'SAFETY_EQUIPMENT', \n",
    "    'AIRBAG_DEPLOYED',  \n",
    "    'time_bins',\n",
    "]]\n",
    "\n",
    "X_train = pd.concat([X_train_nums,X_train_cats] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cheking the distribution of targets\n",
    "\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Pipeline for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as imbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and One Hot Encoding \n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "                \n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(drop='first',\n",
    "                         sparse=False))\n",
    "])\n",
    "\n",
    "trans = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_pipeline, X_train_nums.columns),\n",
    "    ('categorical', categorical_pipeline, X_train_cats.columns)\n",
    "])\n",
    "\n",
    "# Under & Over Sampling    \n",
    "over = SMOTE(sampling_strategy = 0.7)\n",
    "under = RandomUnderSampler(sampling_strategy = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model with Pipeline\n",
    "LR_pipe = imbPipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('over' , over),\n",
    "    ('under', under),\n",
    "    ('LR', LogisticRegression(max_iter = 1e3, penalty = 'none', random_state = 42))\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object with different hyperparameters\n",
    "parameters_LR = {\n",
    "    'LR__C': [1, 10, 100],\n",
    "    'LR__solver': ['lbfgs','sag', 'saga']\n",
    "}\n",
    "\n",
    "cv_LR = GridSearchCV(LR_pipe, param_grid=parameters_LR)\n",
    "\n",
    "cv_LR.fit(X_train, y_train)\n",
    "\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Predict the label with the best model\n",
    "y_pred_LR = cv_LR.predict(X_test)\n",
    "print(y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree Model with Pipeline\n",
    "DT_pipe = imbPipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('over' , over),\n",
    "    ('under', under),\n",
    "    ('DT', DecisionTreeClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "parameters_DT = {\n",
    "    'DT__max_depth': [2, 4, 6, 8],\n",
    "    'DT__min_samples_split': [5, 10, 15],\n",
    "    'DT__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "cv_DT = GridSearchCV(DT_pipe, param_grid=parameters_DT)\n",
    "\n",
    "cv_DT.fit(X_train, y_train)\n",
    "\n",
    "# Predict the label with the best model\n",
    "y_pred_DT = cv_DT.predict(X_test)\n",
    "print(y_pred_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model with Pipeline\n",
    "KNN_pipe = imbPipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('over' , over),\n",
    "    ('under', under),\n",
    "    ('KNN', KNeighborsClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object with different hyperparameters\n",
    "parameters_KNN = {\n",
    "    'KNN__n_neighbors': [1, 3, 5, 7],\n",
    "    'KNN__metric': ['minkowski', 'manhattan'],\n",
    "    'KNN__weights': ['uniform', 'distance'],\n",
    "}\n",
    "\n",
    "cv_KNN = GridSearchCV(KNN_pipe, param_grid=parameters_KNN)\n",
    "\n",
    "cv_KNN.fit(X_train, y_train)\n",
    "\n",
    "# Predict the label with the best model\n",
    "y_pred_KNN = cv_KNN.predict(X_test)\n",
    "print(y_pred_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forrest Model with Pipeline\n",
    "RF_pipe = imbPipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('over' , over),\n",
    "    ('under', under),\n",
    "    ('RF', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "# Create the GridSearchCV object with different hyperparameters\n",
    "parameters = {\n",
    "    'RF__n_estimators': [10, 100, 1000],\n",
    "    'RF__max_depth': [2, 4, 6, 8],\n",
    "    'RF__min_samples_split': [5, 10, 15],\n",
    "    'RF__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "cv_RF = GridSearchCV(RF_pipe, param_grid=parameters_RF)\n",
    "\n",
    "cv_RF.fit(X_train, y_train)\n",
    "\n",
    "# Predict the label with the best model\n",
    "y_pred_RF = cv_RF.predict(X_test)\n",
    "print(y_pred_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
